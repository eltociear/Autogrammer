import { LlamaCPPError, LlamaCPPResponse, } from "../types.js";

export const makeMockLlamaCPPResponse = (r: Partial<LlamaCPPResponse>): LlamaCPPResponse => ({
  content: '',
  generation_settings: {
    "dynatemp_exponent": 0,
    "dynatemp_range": 0,
    "frequency_penalty": 0,
    "grammar": '',
    "ignore_eos": false,
    "logit_bias": [],
    "min_keep": 0,
    "min_p": 0,
    "mirostat": 0,
    "mirostat_eta": 0,
    "mirostat_tau": 0,
    "model": '',
    "n_ctx": 0,
    "n_keep": 0,
    "n_predict": 0,
    "n_probs": 0,
    "penalize_nl": false,
    "penalty_prompt_tokens": [],
    "presence_penalty": 0,
    "repeat_last_n": 0,
    "repeat_penalty": 0,
    "samplers": [],
    "seed": [],
    "stop": [],
    "stream": false,
    "temperature": 0,
    "tfs_z": 0,
    "top_k": 0,
    "top_p": 0,
    "typical_p": 0,
    "use_penalty_prompt_tokens": false,
    ...r.generation_settings,
  },
  "id_slot": 0,
  "model": '',
  "prompt": '',
  "stop": false,
  "stopped_eos": false,
  "stopped_limit": false,
  "stopped_word": false,
  "stopping_word": '',
  "timings": {
    "predicted_ms": 0,
    "predicted_n": 0,
    "predicted_per_second": 0,
    "predicted_per_token_ms": 0,
    "prompt_ms": 0,
    "prompt_n": 0,
    "prompt_per_second": 0,
    "prompt_per_token_ms": 0,
    ...r.timings,
  },
  "tokens_cached": 0,
  "tokens_evaluated": 0,
  "tokens_predicted": 0,
  "truncated": false,
  ...r,
});

export const makeMockLlamaCPPError = (error: Partial<LlamaCPPError> = {}): LlamaCPPError => ({
  code: 0,
  message: 'message',
  type: 'type',
  ...error,
});
